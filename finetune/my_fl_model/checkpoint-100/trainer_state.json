{
  "best_metric": 2.261441469192505,
  "best_model_checkpoint": "./my_fl_model/checkpoint-100",
  "epoch": 0.49504950495049505,
  "eval_steps": 50,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0049504950495049506,
      "grad_norm": 9.011418342590332,
      "learning_rate": 4.998766400914329e-05,
      "loss": 2.3093,
      "step": 1
    },
    {
      "epoch": 0.009900990099009901,
      "grad_norm": 9.488789558410645,
      "learning_rate": 4.995066821070679e-05,
      "loss": 2.1755,
      "step": 2
    },
    {
      "epoch": 0.01485148514851485,
      "grad_norm": 10.053448677062988,
      "learning_rate": 4.9889049115077005e-05,
      "loss": 2.1498,
      "step": 3
    },
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 8.610875129699707,
      "learning_rate": 4.980286753286195e-05,
      "loss": 2.4438,
      "step": 4
    },
    {
      "epoch": 0.024752475247524754,
      "grad_norm": 12.172137260437012,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 2.1814,
      "step": 5
    },
    {
      "epoch": 0.0297029702970297,
      "grad_norm": 10.99435043334961,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 2.1098,
      "step": 6
    },
    {
      "epoch": 0.034653465346534656,
      "grad_norm": 8.25548267364502,
      "learning_rate": 4.939791904846869e-05,
      "loss": 2.3425,
      "step": 7
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 10.697144508361816,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 2.2692,
      "step": 8
    },
    {
      "epoch": 0.04455445544554455,
      "grad_norm": 9.627314567565918,
      "learning_rate": 4.900734214192358e-05,
      "loss": 2.0264,
      "step": 9
    },
    {
      "epoch": 0.04950495049504951,
      "grad_norm": 7.849071979522705,
      "learning_rate": 4.877641290737884e-05,
      "loss": 2.2074,
      "step": 10
    },
    {
      "epoch": 0.054455445544554455,
      "grad_norm": 8.058116912841797,
      "learning_rate": 4.852201922385564e-05,
      "loss": 2.38,
      "step": 11
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 8.866073608398438,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 2.4019,
      "step": 12
    },
    {
      "epoch": 0.06435643564356436,
      "grad_norm": 5.74725341796875,
      "learning_rate": 4.794386564209953e-05,
      "loss": 2.0627,
      "step": 13
    },
    {
      "epoch": 0.06930693069306931,
      "grad_norm": 10.771986961364746,
      "learning_rate": 4.762067631165049e-05,
      "loss": 2.3939,
      "step": 14
    },
    {
      "epoch": 0.07425742574257425,
      "grad_norm": 8.755023956298828,
      "learning_rate": 4.72751631047092e-05,
      "loss": 2.2843,
      "step": 15
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 8.708998680114746,
      "learning_rate": 4.690766700109659e-05,
      "loss": 2.1777,
      "step": 16
    },
    {
      "epoch": 0.08415841584158416,
      "grad_norm": 6.217965602874756,
      "learning_rate": 4.65185506750986e-05,
      "loss": 2.2014,
      "step": 17
    },
    {
      "epoch": 0.0891089108910891,
      "grad_norm": 9.066959381103516,
      "learning_rate": 4.610819813755038e-05,
      "loss": 2.304,
      "step": 18
    },
    {
      "epoch": 0.09405940594059406,
      "grad_norm": 10.574125289916992,
      "learning_rate": 4.567701435686404e-05,
      "loss": 2.391,
      "step": 19
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 7.4135661125183105,
      "learning_rate": 4.522542485937369e-05,
      "loss": 2.3321,
      "step": 20
    },
    {
      "epoch": 0.10396039603960396,
      "grad_norm": 12.415131568908691,
      "learning_rate": 4.4753875309392266e-05,
      "loss": 2.2043,
      "step": 21
    },
    {
      "epoch": 0.10891089108910891,
      "grad_norm": 5.995645999908447,
      "learning_rate": 4.426283106939474e-05,
      "loss": 2.2789,
      "step": 22
    },
    {
      "epoch": 0.11386138613861387,
      "grad_norm": 7.965423583984375,
      "learning_rate": 4.375277674076149e-05,
      "loss": 2.2487,
      "step": 23
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 7.939000606536865,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 2.3677,
      "step": 24
    },
    {
      "epoch": 0.12376237623762376,
      "grad_norm": 6.63078498840332,
      "learning_rate": 4.267766952966369e-05,
      "loss": 2.3388,
      "step": 25
    },
    {
      "epoch": 0.12871287128712872,
      "grad_norm": 6.965202331542969,
      "learning_rate": 4.211367764821722e-05,
      "loss": 2.0541,
      "step": 26
    },
    {
      "epoch": 0.13366336633663367,
      "grad_norm": 6.467202186584473,
      "learning_rate": 4.1532796633091296e-05,
      "loss": 2.2663,
      "step": 27
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 7.09379768371582,
      "learning_rate": 4.093559974371725e-05,
      "loss": 2.3581,
      "step": 28
    },
    {
      "epoch": 0.14356435643564355,
      "grad_norm": 6.602327346801758,
      "learning_rate": 4.0322676341324415e-05,
      "loss": 2.1887,
      "step": 29
    },
    {
      "epoch": 0.1485148514851485,
      "grad_norm": 6.888584613800049,
      "learning_rate": 3.969463130731183e-05,
      "loss": 2.1811,
      "step": 30
    },
    {
      "epoch": 0.15346534653465346,
      "grad_norm": 9.575983047485352,
      "learning_rate": 3.905208444630327e-05,
      "loss": 2.2027,
      "step": 31
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 8.806585311889648,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 2.1078,
      "step": 32
    },
    {
      "epoch": 0.16336633663366337,
      "grad_norm": 7.7147135734558105,
      "learning_rate": 3.7726035393759285e-05,
      "loss": 2.2478,
      "step": 33
    },
    {
      "epoch": 0.16831683168316833,
      "grad_norm": 6.710037708282471,
      "learning_rate": 3.704384185254288e-05,
      "loss": 2.1564,
      "step": 34
    },
    {
      "epoch": 0.17326732673267325,
      "grad_norm": 6.84800386428833,
      "learning_rate": 3.634976249348867e-05,
      "loss": 2.0889,
      "step": 35
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 10.215738296508789,
      "learning_rate": 3.564448228912682e-05,
      "loss": 2.176,
      "step": 36
    },
    {
      "epoch": 0.18316831683168316,
      "grad_norm": 6.110415935516357,
      "learning_rate": 3.4928697265869515e-05,
      "loss": 2.1282,
      "step": 37
    },
    {
      "epoch": 0.18811881188118812,
      "grad_norm": 6.5940423011779785,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 2.1078,
      "step": 38
    },
    {
      "epoch": 0.19306930693069307,
      "grad_norm": 6.811112403869629,
      "learning_rate": 3.346844800613229e-05,
      "loss": 2.4204,
      "step": 39
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 13.20567512512207,
      "learning_rate": 3.272542485937369e-05,
      "loss": 2.3541,
      "step": 40
    },
    {
      "epoch": 0.20297029702970298,
      "grad_norm": 6.49724006652832,
      "learning_rate": 3.1974777650980735e-05,
      "loss": 2.297,
      "step": 41
    },
    {
      "epoch": 0.2079207920792079,
      "grad_norm": 6.041210651397705,
      "learning_rate": 3.121724717912138e-05,
      "loss": 2.3232,
      "step": 42
    },
    {
      "epoch": 0.21287128712871287,
      "grad_norm": 6.723599910736084,
      "learning_rate": 3.045358103491357e-05,
      "loss": 2.3005,
      "step": 43
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 9.204814910888672,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 2.441,
      "step": 44
    },
    {
      "epoch": 0.22277227722772278,
      "grad_norm": 6.608050346374512,
      "learning_rate": 2.8910861626005776e-05,
      "loss": 2.3675,
      "step": 45
    },
    {
      "epoch": 0.22772277227722773,
      "grad_norm": 9.081682205200195,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 2.0124,
      "step": 46
    },
    {
      "epoch": 0.23267326732673269,
      "grad_norm": 7.908810615539551,
      "learning_rate": 2.7352707832962865e-05,
      "loss": 2.4064,
      "step": 47
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 16.916839599609375,
      "learning_rate": 2.656976298823284e-05,
      "loss": 2.2258,
      "step": 48
    },
    {
      "epoch": 0.24257425742574257,
      "grad_norm": 8.424141883850098,
      "learning_rate": 2.578526897695321e-05,
      "loss": 2.3286,
      "step": 49
    },
    {
      "epoch": 0.24752475247524752,
      "grad_norm": 5.953058242797852,
      "learning_rate": 2.5e-05,
      "loss": 2.1559,
      "step": 50
    },
    {
      "epoch": 0.24752475247524752,
      "eval_loss": 2.2692646980285645,
      "eval_runtime": 0.4969,
      "eval_samples_per_second": 342.118,
      "eval_steps_per_second": 22.137,
      "step": 50
    },
    {
      "epoch": 0.2524752475247525,
      "grad_norm": 7.34975528717041,
      "learning_rate": 2.4214731023046793e-05,
      "loss": 2.1516,
      "step": 51
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 5.930501937866211,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 2.2548,
      "step": 52
    },
    {
      "epoch": 0.2623762376237624,
      "grad_norm": 6.427763938903809,
      "learning_rate": 2.2647292167037144e-05,
      "loss": 2.2303,
      "step": 53
    },
    {
      "epoch": 0.26732673267326734,
      "grad_norm": 7.456149578094482,
      "learning_rate": 2.186666916089239e-05,
      "loss": 2.2848,
      "step": 54
    },
    {
      "epoch": 0.2722772277227723,
      "grad_norm": 6.707995891571045,
      "learning_rate": 2.1089138373994223e-05,
      "loss": 2.335,
      "step": 55
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 7.3471598625183105,
      "learning_rate": 2.031546713535688e-05,
      "loss": 2.4026,
      "step": 56
    },
    {
      "epoch": 0.28217821782178215,
      "grad_norm": 10.31495189666748,
      "learning_rate": 1.9546418965086442e-05,
      "loss": 1.9611,
      "step": 57
    },
    {
      "epoch": 0.2871287128712871,
      "grad_norm": 6.930203437805176,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 2.2452,
      "step": 58
    },
    {
      "epoch": 0.29207920792079206,
      "grad_norm": 10.757501602172852,
      "learning_rate": 1.802522234901927e-05,
      "loss": 2.226,
      "step": 59
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 6.848784446716309,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 2.3191,
      "step": 60
    },
    {
      "epoch": 0.30198019801980197,
      "grad_norm": 12.571759223937988,
      "learning_rate": 1.6531551993867717e-05,
      "loss": 2.2709,
      "step": 61
    },
    {
      "epoch": 0.3069306930693069,
      "grad_norm": 5.545073509216309,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 2.03,
      "step": 62
    },
    {
      "epoch": 0.3118811881188119,
      "grad_norm": 5.794661998748779,
      "learning_rate": 1.5071302734130489e-05,
      "loss": 2.2194,
      "step": 63
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 6.993405818939209,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 2.253,
      "step": 64
    },
    {
      "epoch": 0.3217821782178218,
      "grad_norm": 4.879414081573486,
      "learning_rate": 1.3650237506511331e-05,
      "loss": 2.1359,
      "step": 65
    },
    {
      "epoch": 0.32673267326732675,
      "grad_norm": 8.000914573669434,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 2.2503,
      "step": 66
    },
    {
      "epoch": 0.3316831683168317,
      "grad_norm": 14.57585620880127,
      "learning_rate": 1.2273964606240718e-05,
      "loss": 1.9453,
      "step": 67
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 7.357921600341797,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 2.297,
      "step": 68
    },
    {
      "epoch": 0.3415841584158416,
      "grad_norm": 7.645018100738525,
      "learning_rate": 1.0947915553696742e-05,
      "loss": 2.0849,
      "step": 69
    },
    {
      "epoch": 0.3465346534653465,
      "grad_norm": 7.8420867919921875,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 2.0694,
      "step": 70
    },
    {
      "epoch": 0.35148514851485146,
      "grad_norm": 7.328774452209473,
      "learning_rate": 9.677323658675594e-06,
      "loss": 2.1331,
      "step": 71
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 7.0440473556518555,
      "learning_rate": 9.064400256282757e-06,
      "loss": 2.2334,
      "step": 72
    },
    {
      "epoch": 0.3613861386138614,
      "grad_norm": 7.133865833282471,
      "learning_rate": 8.467203366908707e-06,
      "loss": 2.373,
      "step": 73
    },
    {
      "epoch": 0.36633663366336633,
      "grad_norm": 6.940016746520996,
      "learning_rate": 7.886322351782783e-06,
      "loss": 2.2016,
      "step": 74
    },
    {
      "epoch": 0.3712871287128713,
      "grad_norm": 6.947928428649902,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 2.192,
      "step": 75
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 6.981723308563232,
      "learning_rate": 6.775784314464717e-06,
      "loss": 2.3558,
      "step": 76
    },
    {
      "epoch": 0.3811881188118812,
      "grad_norm": 7.419003486633301,
      "learning_rate": 6.247223259238511e-06,
      "loss": 2.2175,
      "step": 77
    },
    {
      "epoch": 0.38613861386138615,
      "grad_norm": 5.493416786193848,
      "learning_rate": 5.737168930605272e-06,
      "loss": 2.3821,
      "step": 78
    },
    {
      "epoch": 0.3910891089108911,
      "grad_norm": 7.75048828125,
      "learning_rate": 5.24612469060774e-06,
      "loss": 2.1326,
      "step": 79
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 9.9536714553833,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 2.2332,
      "step": 80
    },
    {
      "epoch": 0.400990099009901,
      "grad_norm": 7.4132890701293945,
      "learning_rate": 4.322985643135952e-06,
      "loss": 2.1657,
      "step": 81
    },
    {
      "epoch": 0.40594059405940597,
      "grad_norm": 6.621345043182373,
      "learning_rate": 3.891801862449629e-06,
      "loss": 2.1681,
      "step": 82
    },
    {
      "epoch": 0.41089108910891087,
      "grad_norm": 7.057538032531738,
      "learning_rate": 3.4814493249014116e-06,
      "loss": 2.1156,
      "step": 83
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 8.448444366455078,
      "learning_rate": 3.092332998903416e-06,
      "loss": 2.3359,
      "step": 84
    },
    {
      "epoch": 0.4207920792079208,
      "grad_norm": 7.359184741973877,
      "learning_rate": 2.7248368952908053e-06,
      "loss": 2.2008,
      "step": 85
    },
    {
      "epoch": 0.42574257425742573,
      "grad_norm": 5.834601402282715,
      "learning_rate": 2.379323688349516e-06,
      "loss": 2.3316,
      "step": 86
    },
    {
      "epoch": 0.4306930693069307,
      "grad_norm": 11.182462692260742,
      "learning_rate": 2.0561343579004715e-06,
      "loss": 2.2767,
      "step": 87
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 7.553009033203125,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 2.2099,
      "step": 88
    },
    {
      "epoch": 0.4405940594059406,
      "grad_norm": 7.34886360168457,
      "learning_rate": 1.4779807761443636e-06,
      "loss": 2.2457,
      "step": 89
    },
    {
      "epoch": 0.44554455445544555,
      "grad_norm": 6.394495487213135,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 2.3729,
      "step": 90
    },
    {
      "epoch": 0.4504950495049505,
      "grad_norm": 7.128702163696289,
      "learning_rate": 9.926578580764234e-07,
      "loss": 2.1724,
      "step": 91
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 6.618099212646484,
      "learning_rate": 7.854209717842231e-07,
      "loss": 2.2272,
      "step": 92
    },
    {
      "epoch": 0.4603960396039604,
      "grad_norm": 7.281844139099121,
      "learning_rate": 6.020809515313142e-07,
      "loss": 2.3021,
      "step": 93
    },
    {
      "epoch": 0.46534653465346537,
      "grad_norm": 6.5708513259887695,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 2.0749,
      "step": 94
    },
    {
      "epoch": 0.47029702970297027,
      "grad_norm": 7.772263526916504,
      "learning_rate": 3.077914851215585e-07,
      "loss": 2.0707,
      "step": 95
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 7.61089563369751,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 2.2128,
      "step": 96
    },
    {
      "epoch": 0.4801980198019802,
      "grad_norm": 8.826483726501465,
      "learning_rate": 1.109508849230001e-07,
      "loss": 2.2917,
      "step": 97
    },
    {
      "epoch": 0.48514851485148514,
      "grad_norm": 6.911520481109619,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 2.3519,
      "step": 98
    },
    {
      "epoch": 0.4900990099009901,
      "grad_norm": 6.911769866943359,
      "learning_rate": 1.233599085671e-08,
      "loss": 2.3954,
      "step": 99
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 8.46812629699707,
      "learning_rate": 0.0,
      "loss": 2.3081,
      "step": 100
    },
    {
      "epoch": 0.49504950495049505,
      "eval_loss": 2.261441469192505,
      "eval_runtime": 0.5243,
      "eval_samples_per_second": 324.265,
      "eval_steps_per_second": 20.982,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 114789226905600.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
